{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ íŠœë‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í”„ë¡¬í”„íŠ¸ëŠ” íŠ¹ì • ì‘ì—…ì— ë§ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ì˜ ë™ì‘ì„ ì•ˆë‚´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ íŠœë‹ì€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì— ìƒˆë¡œ ì¶”ê°€ëœ í”„ë¡¬í”„íŠ¸ í† í°ë§Œ í›ˆë ¨í•˜ê³  ì—…ë°ì´íŠ¸í•˜ëŠ” ì¶”ê°€ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ê°€ì¤‘ì¹˜ê°€ ê³ ì •ëœ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ê³ , ê° ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•´ ë³„ë„ì˜ ëª¨ë¸ì„ ì™„ì „íˆ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ëŒ€ì‹  ë” ì‘ì€ í”„ë¡¬í”„íŠ¸ ë§¤ê°œë³€ìˆ˜ ì§‘í•©ì„ í›ˆë ¨í•˜ê³  ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ ì ì  ë” ì»¤ì§ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ íŠœë‹ì€ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìœ¼ë©° ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ê°€ í™•ì¥ë¨ì— ë”°ë¼ ê²°ê³¼ë„ í›¨ì”¬ ì¢‹ì•„ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ğŸ’¡ í”„ë¡¬í”„íŠ¸ íŠœë‹ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ë§¤ê°œë³€ìˆ˜ íš¨ìœ¨ì ì¸ í”„ë¡¬í”„íŠ¸ íŠœë‹ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ì˜ í˜](https://arxiv.org/abs/2104.08691)ì„ ì½ì–´ë³´ì„¸ìš”.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ì´ ê°€ì´ë“œì—ì„œëŠ” í”„ë¡¬í”„íŠ¸ íŠœë‹ì„ ì ìš©í•˜ì—¬ [RAFT](https://huggingface.co/datasets/ought/raft) ë°ì´í„° ì„¸íŠ¸ì˜ `twitter_complaints` í•˜ìœ„ ì§‘í•©ì—ì„œ [`bloomz-560m`](https://huggingface.co/bigscience/bloomz-560m) ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "\n",
    "ì‹œì‘í•˜ê¸° ì „ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n",
    "\n",
    "```bash\n",
    "!pip install -q peft transformers datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €, í›ˆë ¨í•  ë°ì´í„° ì„¸íŠ¸ ë° ë°ì´í„° ì„¸íŠ¸ ì—´, ì¼ë¶€ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°, [PromptTuningConfig](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig)ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. [PromptTuningConfig](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig)ì—ëŠ” ì‘ì—… ìœ í˜•, í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ì„ ì´ˆê¸°í™”í•  í…ìŠ¤íŠ¸, ê°€ìƒ í† í° ìˆ˜, ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name_or_path = \"bigscience/bloomz-560m\"\n",
    "tokenizer_name_or_path = \"bigscience/bloomz-560m\"\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=8,\n",
    "    prompt_tuning_init_text=\"íŠ¸ìœ—ì´ ë¶ˆë§Œ ì‚¬í•­ì¸ì§€ ì•„ë‹Œì§€ ë¶„ë¥˜í•©ë‹ˆë‹¤.\",\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    ")\n",
    "\n",
    "dataset_name = \"twitter_complaints\"\n",
    "checkpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n",
    "    \"/\", \"_\"\n",
    ")\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 64\n",
    "lr = 3e-2\n",
    "num_epochs = 50\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì„¸íŠ¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ê°€ì´ë“œì—ì„œëŠ” [RAFT](https://huggingface.co/datasets/ought/raft) ë°ì´í„° ì„¸íŠ¸ì˜ `twitter_complaints` í•˜ìœ„ ì§‘í•©ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì´ í•˜ìœ„ ì§‘í•©ì—ëŠ” `complaint` ë˜ëŠ” `no complaint`ë¡œ ë ˆì´ë¸”ì´ ì§€ì •ëœ íŠ¸ìœ—ì´ í¬í•¨ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "dataset[\"train\"][0]\n",
    "{\"Tweet text\": \"@HMRCcustomers No this is my first job\", \"ID\": 0, \"Label\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Label` ì—´ì„ ë” ì½ê¸° ì‰½ê²Œ ë§Œë“¤ë ¤ë©´ `Label` ê°’ì„ í•´ë‹¹ ë ˆì´ë¸” í…ìŠ¤íŠ¸ë¡œ ë°”ê¾¸ê³  `text_label` ì—´ì— ì €ì¥í•©ë‹ˆë‹¤. [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ë³€ê²½ ì‚¬í•­ì„ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— í•œ ë²ˆì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "dataset[\"train\"][0]\n",
    "{\"Tweet text\": \"@HMRCcustomers No this is my first job\", \"ID\": 0, \"Label\": 2, \"text_label\": \"no complaint\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì„¸íŠ¸ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ì„¤ì •í•˜ê³ , ì‹œí€€ìŠ¤ íŒ¨ë”©ì— ì‚¬ìš©í•  ì ì ˆí•œ íŒ¨ë”© í† í°ì„ êµ¬ì„±í•˜ê³ , í† í°í™”ëœ ë ˆì´ë¸”ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n",
    "print(target_max_length)\n",
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`preprocess_function`ì„ ë§Œë“¤ì–´ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì„ í† í°í™”í•©ë‹ˆë‹¤.\n",
    "2. ë°°ì¹˜ì˜ ê° ì˜ˆì œì— ëŒ€í•´ í† í¬ë‚˜ì´ì €ì˜ `pad_token_id`ë¡œ ë ˆì´ë¸”ì„ íŒ¨ë”©í•©ë‹ˆë‹¤.\n",
    "3. ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì„ `model_inputs`ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "4. `labels` ë° `model_inputs`ì— ëŒ€í•œ ë³„ë„ì˜ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "5. ë°°ì¹˜ì˜ ê° ì˜ˆì œë¥¼ ë‹¤ì‹œ ë°˜ë³µí•˜ì—¬ ì…ë ¥ ID, ë ˆì´ë¸”, ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ `max_length`ë¡œ íŒ¨ë”©í•˜ê³  PyTorch í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ `preprocess_function`ì„ ì „ì²´ ë°ì´í„° ì„¸íŠ¸ì— ì ìš©í•©ë‹ˆë‹¤. ëª¨ë¸ì— ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì—´ì„ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train` ë° `eval` ë°ì´í„° ì„¸íŠ¸ì—ì„œ [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)ë¥¼ ë§Œë“­ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ì˜ ìƒ˜í”Œì´ CPUì— ìˆëŠ” ê²½ìš° í›ˆë ¨ ì¤‘ GPUë¡œì˜ ë°ì´í„° ì „ì†¡ ì†ë„ë¥¼ ë†’ì´ë ¤ë©´ `pin_memory=True`ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í›ˆë ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ ì„¤ì •í•˜ê³  í›ˆë ¨ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ê±°ì˜ ë‹¤ ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "[AutoModelForCausalLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCausalLM)ì—ì„œ ê¸°ë³¸ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  `peft_config`ì™€ í•¨ê»˜ `get_peft_model()` í•¨ìˆ˜ì— ì „ë‹¬í•˜ì—¬ [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel)ì„ ë§Œë“­ë‹ˆë‹¤. ìƒˆ [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel)ì˜ í›ˆë ¨ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¸ì‡„í•˜ì—¬ ì›ë˜ ëª¨ë¸ì˜ ì „ì²´ ë§¤ê°œë³€ìˆ˜ë¥¼ í›ˆë ¨í•˜ëŠ” ê²ƒë³´ë‹¤ ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ì¸ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model.print_trainable_parameters())\n",
    "\"trainable params: 8192 || all params: 559222784 || trainable%: 0.0014648902430985358\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ GPUë¡œ ì˜®ê¸´ ë‹¤ìŒ í›ˆë ¨ ë£¨í”„ë¥¼ ì‘ì„±í•˜ì—¬ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ê³µìœ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì›í•˜ëŠ” ê²½ìš° Hubì—ì„œ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ê³  ë©”ì‹œì§€ê°€ í‘œì‹œë˜ë©´ í† í°ì„ ì…ë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[push_to_hub](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.push_to_hub) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Hubì˜ ëª¨ë¸ ë¦¬í¬ì§€í† ë¦¬ì— ëª¨ë¸ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"your-name/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\"\n",
    "model.push_to_hub(\"your-name/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì´ ì—…ë¡œë“œë˜ë©´ ëª¨ë¸ íŒŒì¼ í¬ê¸°ê°€ 33.5kBì— ë¶ˆê³¼í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ë¡ ì„ ìœ„í•´ ìƒ˜í”Œ ì…ë ¥ì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì—…ë¡œë“œí•œ ë¦¬í¬ì§€í† ë¦¬ë¥¼ ë³´ë©´ `adapter_config.json` íŒŒì¼ì´ í‘œì‹œë©ë‹ˆë‹¤. ì´ íŒŒì¼ì„ [PeftConfig](https://huggingface.co/docs/peft/main/en/package_reference/config#peft.PeftConfig)ì— ë¡œë“œí•˜ì—¬ `peft_type` ë° `task_type`ì„ ì§€ì •í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ íŠœë‹ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ì™€ êµ¬ì„±ì„ [from_pretrained()](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel.from_pretrained)ì— ë¡œë“œí•˜ì—¬ [PeftModel](https://huggingface.co/docs/peft/main/en/package_reference/peft_model#peft.PeftModel)ì„ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = \"stevhliu/bloomz-560m_PROMPT_TUNING_CAUSAL_LM\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "íŠ¸ìœ—ì„ ê°€ì ¸ì™€ í† í°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    f'{text_column} : {\"@nationalgridus ë¬¼ì´ ë‚˜ì˜¤ì§€ ì•Šê³  ìš”ê¸ˆì€ í˜„ì¬ ì§€ë¶ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•´ ì¡°ì¹˜ë¥¼ ì·¨í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?\"} Label : ',\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ GPUì— ë†“ê³  ì˜ˆì¸¡ëœ ë ˆì´ë¸”ì„ *ìƒì„±*í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n",
    "[\n",
    "    \"Tweet text : @nationalgridus ë¬¼ì´ ë‚˜ì˜¤ì§€ ì•Šê³  ìš”ê¸ˆì€ í˜„ì¬ ì§€ë¶ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•´ ì¡°ì¹˜ë¥¼ ì·¨í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ? Label : complaint\"\n",
    "]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
