{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6LLOPZouLg"
   },
   "source": [
    "# Hugging Face TRL을 사용하여 LoRA 어댑터로 LLM 미세 조정하는 방법\n",
    "\n",
    "이 노트북은 LoRA(Low-Rank Adaptation) 어댑터를 사용하여 대규모 언어 모델을 효율적으로 미세 조정하는 방법을 보여줍니다. LoRA는 다음과 같은 매개변수 효율적인 미세 조정 기법입니다.\n",
    "- 사전 훈련된 모델 가중치 고정\n",
    "- 어텐션 레이어에 작은 훈련 가능한 순위 분해 행렬 추가\n",
    "- 일반적으로 훈련 가능한 매개변수를 약 90% 줄입니다.\n",
    "- 메모리 효율적이면서 모델 성능 유지\n",
    "\n",
    "다룰 내용:\n",
    "1. 개발 환경 및 LoRA 구성 설정\n",
    "2. 어댑터 훈련을 위한 데이터 세트 생성 및 준비\n",
    "3. `trl` 및 `SFTTrainer`와 LoRA 어댑터를 사용하여 미세 조정\n",
    "4. 모델 테스트 및 어댑터 병합(선택 사항)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqd9BXgouLi"
   },
   "source": [
    "## 1. 개발 환경 설정\n",
    "\n",
    "첫 번째 단계는 trl, transformers, datasets를 포함하여 Hugging Face 라이브러리와 Pytorch를 설치하는 것입니다. 아직 trl에 대해 들어본 적이 없다면 걱정하지 마십시오. 이는 transformers 및 datasets 위에 있는 새로운 라이브러리로, 개방형 LLM을 미세 조정, rlhf, 정렬하는 것을 더 쉽게 만듭니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKvGVxImouLi"
   },
   "outputs": [],
   "source": [
    "# Google Colab에서 요구 사항 설치\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Hugging Face에 인증\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# 편의를 위해 허브 토큰을 HF_TOKEN으로 환경 변수에 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHUzfwpKouLk"
   },
   "source": [
    "## 2. 데이터 세트 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z4p6Bvo7ouLk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 샘플 데이터 세트 로드\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: path 및 name 매개변수를 사용하여 데이터 세트 및 구성을 정의합니다.\n",
    "dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOhJdtsouLk"
   },
   "source": [
    "## 3. `trl` 및 LoRA가 포함된 `SFTTrainer`를 사용하여 LLM 미세 조정\n",
    "\n",
    "`trl`의 [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer)는 [PEFT](https://huggingface.co/docs/peft/en/index) 라이브러리를 통해 LoRA 어댑터와의 통합을 제공합니다. 이 설정의 주요 이점은 다음과 같습니다.\n",
    "\n",
    "1. **메모리 효율성**:\n",
    "   - 어댑터 매개변수만 GPU 메모리에 저장됩니다.\n",
    "   - 기본 모델 가중치는 고정된 상태로 유지되며 더 낮은 정밀도로 로드할 수 있습니다.\n",
    "   - 소비자 GPU에서 대규모 모델을 미세 조정할 수 있습니다.\n",
    "\n",
    "2. **훈련 기능**:\n",
    "   - 최소한의 설정으로 기본 PEFT/LoRA 통합\n",
    "   - 메모리 효율성을 더욱 향상시키는 QLoRA(양자화된 LoRA) 지원\n",
    "\n",
    "3. **어댑터 관리**:\n",
    "   - 체크포인트 중 어댑터 가중치 저장\n",
    "   - 어댑터를 기본 모델에 다시 병합하는 기능\n",
    "\n",
    "예제에서는 LoRA를 사용하며, LoRA와 4비트 양자화를 결합하여 성능 저하 없이 메모리 사용량을 더욱 줄입니다. 설정에는 몇 가지 구성 단계만 필요합니다.\n",
    "1. LoRA 구성 정의(순위, 알파, 드롭아웃)\n",
    "2. PEFT 구성으로 SFTTrainer 생성\n",
    "3. 어댑터 가중치 훈련 및 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 가져오기\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# 채팅 형식 설정\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 미세 조정을 저장하거나 업로드할 이름 설정\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbuVArTHouLk"
   },
   "source": [
    "`SFTTrainer`는 `peft`와의 기본 통합을 지원하므로 LoRA 등을 사용하여 LLM을 효율적으로 조정하는 것이 매우 쉽습니다. `LoraConfig`를 만들고 트레이너에게 제공하기만 하면 됩니다.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>연습: 미세 조정을 위한 LoRA 매개변수 정의</h2>\n",
    "    <p>Hugging Face 허브에서 데이터 세트를 가져와 모델을 미세 조정합니다. </p>\n",
    "    <p><b>난이도</b></p>\n",
    "    <p>🐢 임의의 미세 조정을 위한 일반적인 매개변수를 사용합니다.</p>\n",
    "    <p>🐕 매개변수를 조정하고 가중치 및 편향에서 검토합니다.</p>\n",
    "    <p>🦁 매개변수를 조정하고 추론 결과의 변화를 보여줍니다.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: LoRA 매개변수 구성\n",
    "# r: LoRA 업데이트 행렬의 순위 차원(작을수록 압축률이 높아짐)\n",
    "rank_dimension = 6\n",
    "# lora_alpha: LoRA 레이어의 스케일링 팩터(높을수록 적응력이 강해짐)\n",
    "lora_alpha = 8\n",
    "# lora_dropout: LoRA 레이어의 드롭아웃 확률(과적합 방지에 도움)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # 순위 차원 - 일반적으로 4-32 사이\n",
    "    lora_alpha=lora_alpha,  # LoRA 스케일링 팩터 - 일반적으로 순위의 2배\n",
    "    lora_dropout=lora_dropout,  # LoRA 레이어의 드롭아웃 확률\n",
    "    bias=\"none\",  # LoRA의 편향 유형. 해당 편향은 훈련 중에 업데이트됩니다.\n",
    "    target_modules=\"all-linear\",  # LoRA를 적용할 모듈\n",
    "    task_type=\"CAUSAL_LM\",  # 모델 아키텍처의 작업 유형\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5NUDPcaouLl"
   },
   "source": [
    "훈련을 시작하기 전에 사용할 하이퍼파라미터(`TrainingArguments`)를 정의해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [],
   "source": [
    "# 훈련 구성\n",
    "# QLoRA 논문 권장 사항에 기반한 하이퍼파라미터\n",
    "args = SFTConfig(\n",
    "    # 출력 설정\n",
    "    output_dir=finetune_name,  # 모델 체크포인트를 저장할 디렉터리\n",
    "    # 훈련 기간\n",
    "    num_train_epochs=1,  # 훈련 에포크 수\n",
    "    # 배치 크기 설정\n",
    "    per_device_train_batch_size=2,  # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,  # 더 큰 유효 배치를 위해 기울기 누적\n",
    "    # 메모리 최적화\n",
    "    gradient_checkpointing=True,  # 메모리 절약을 위해 계산량 절충\n",
    "    # 옵티마이저 설정\n",
    "    optim=\"adamw_torch_fused\",  # 효율성을 위해 융합된 AdamW 사용\n",
    "    learning_rate=2e-4,  # 학습률(QLoRA 논문)\n",
    "    max_grad_norm=0.3,  # 기울기 클리핑 임계값\n",
    "    # 학습률 스케줄\n",
    "    warmup_ratio=0.03,  # 워밍업 단계 비율\n",
    "    lr_scheduler_type=\"constant\",  # 워밍업 후 학습률 일정하게 유지\n",
    "    # 로깅 및 저장\n",
    "    logging_steps=10,  # N 단계마다 메트릭 로깅\n",
    "    save_strategy=\"epoch\",  # 에포크마다 체크포인트 저장\n",
    "    # 정밀도 설정\n",
    "    bf16=True,  # bfloat16 정밀도 사용\n",
    "    # 통합 설정\n",
    "    push_to_hub=False,  # HuggingFace 허브에 푸시하지 않음\n",
    "    report_to=\"none\",  # 외부 로깅 비활성화\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGhR7uFBouLl"
   },
   "source": [
    "이제 모델 훈련을 시작하기 위해 `SFTTrainer`를 만드는 데 필요한 모든 구성 요소를 갖추었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M00Har2douLl"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 1512  # 모델 및 데이터 세트 패킹을 위한 최대 시퀀스 길이\n",
    "\n",
    "# LoRA 구성으로 SFTTrainer 생성\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,  # LoRA 구성\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,  # 효율성을 위해 입력 패킹 활성화\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # 템플릿에서 처리하는 특수 토큰\n",
    "        \"append_concat_token\": False,  # 추가 구분 기호 필요 없음\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_kRN24ouLl"
   },
   "source": [
    "`Trainer` 인스턴스에서 `train()` 메서드를 호출하여 모델 훈련을 시작합니다. 이렇게 하면 훈련 루프가 시작되고 3 에포크 동안 모델이 훈련됩니다. PEFT 방법을 사용하므로 전체 모델이 아닌 조정된 모델 가중치만 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq4nIYqKouLl"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300e5dfbb4b54750b77324345c7591f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=1.6402628521124523, metrics={'train_runtime': 195.2398, 'train_samples_per_second': 1.485, 'train_steps_per_second': 0.369, 'total_flos': 282267289092096.0, 'train_loss': 1.6402628521124523, 'epoch': 0.993103448275862})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 시작, 모델은 허브 및 출력 디렉터리에 자동으로 저장됩니다.\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4HHSYYzouLl"
   },
   "source": [
    "15k 샘플 데이터 세트로 3 에포크 동안 Flash Attention으로 훈련하는 데 `g5.2xlarge`에서 4:14:36이 걸렸습니다. 인스턴스 비용은 `1.21$/h`이므로 총 비용은 약 `5.3$`입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C309KsXjouLl"
   },
   "source": [
    "### LoRA 어댑터를 원본 모델에 병합\n",
    "\n",
    "LoRA를 사용할 때 기본 모델은 고정된 상태로 유지하면서 어댑터 가중치만 훈련합니다. 훈련 중에는 전체 모델 복사본이 아닌 이러한 경량 어댑터 가중치(약 2-10MB)만 저장합니다. 그러나 배포를 위해 다음과 같은 이유로 어댑터를 기본 모델에 다시 병합할 수 있습니다.\n",
    "\n",
    "1. **간소화된 배포**: 기본 모델 + 어댑터 대신 단일 모델 파일\n",
    "2. **추론 속도**: 어댑터 계산 오버헤드 없음\n",
    "3. **프레임워크 호환성**: 서빙 프레임워크와의 호환성 향상\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "# CPU에서 PEFT 모델 로드\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# LoRA 및 기본 모델 병합 및 저장\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yO6E9quouLl"
   },
   "source": [
    "## 3. 모델 테스트 및 추론 실행\n",
    "\n",
    "훈련이 끝나면 모델을 테스트합니다. 원본 데이터 세트에서 다른 샘플을 로드하고 간단한 루프와 정확도를 메트릭으로 사용하여 해당 샘플에서 모델을 평가합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>보너스 연습: LoRA 어댑터 로드</h2>\n",
    "    <p>예제 노트북에서 배운 내용을 사용하여 훈련된 LoRA 어댑터를 추론용으로 로드합니다.</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "I5B494OdouLl"
   },
   "outputs": [],
   "source": [
    "# 다시 메모리 해제\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1UhohVdouLl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# PEFT 어댑터로 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=merged_model, tokenizer=tokenizer, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99uFDAuuouLl"
   },
   "source": [
    "일부 프롬프트 샘플을 테스트하고 모델 성능을 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-shSmUbvouLl",
    "outputId": "16d97c61-3b31-4040-c780-3c4de75c3824"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"What is the difference between a fruit and a vegetable? Give examples of each.\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
