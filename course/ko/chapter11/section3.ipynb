{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFTTrainer로 지도 미세 조정\n",
    "\n",
    "이 노트북은 `trl` 라이브러리의 `SFTTrainer`를 사용하여 `HuggingFaceTB/SmolLM2-135M` 모델을 미세 조정하는 방법을 보여줍니다. 노트북 셀이 실행되어 모델을 미세 조정합니다. 다양한 데이터 세트를 시도하여 난이도를 선택할 수 있습니다.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>연습: SFTTrainer로 SmolLM2 미세 조정</h2>\n",
    "    <p>Hugging Face 허브에서 데이터 세트를 가져와 모델을 미세 조정합니다. </p> \n",
    "    <p><b>난이도</b></p>\n",
    "    <p>🐢 `HuggingFaceTB/smoltalk` 데이터 세트를 사용합니다.</p>\n",
    "    <p>🐕 `bigcode/the-stack-smol` 데이터 세트를 시도하고 특정 하위 집합 `data/python`에서 코드 생성 모델을 미세 조정합니다.</p>\n",
    "    <p>🦁 관심 있는 실제 사용 사례와 관련된 데이터 세트를 선택합니다.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab에서 요구 사항 설치\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Hugging Face에 인증\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "# 편의를 위해 허브 토큰을 HF_TOKEN으로 환경 변수에 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 가져오기\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# 채팅 형식 설정\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 미세 조정을 저장하거나 업로드할 이름 설정\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본 모델로 생성\n",
    "\n",
    "여기서는 채팅 템플릿이 없는 기본 모델을 사용해 보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 전에 기본 모델을 테스트합니다.\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# 템플릿으로 형식 지정\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# 응답 생성\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 세트 준비\n",
    "\n",
    "샘플 데이터 세트를 로드하고 훈련용으로 형식을 지정합니다. 데이터 세트는 각 입력이 프롬프트이고 출력이 모델의 예상 응답인 입력-출력 쌍으로 구성되어야 합니다.\n",
    "\n",
    "**TRL은 모델의 채팅 템플릿을 기반으로 입력 메시지 형식을 지정합니다.** `role` 및 `content` 키가 있는 사전 목록으로 표시되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터 세트 로드\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: path 및 name 매개변수를 사용하여 데이터 세트 및 구성을 정의합니다.\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 🦁 데이터 세트가 TRL이 채팅 템플릿으로 변환할 수 있는 형식이 아닌 경우 처리해야 합니다. [모듈](../chat_templates.md)을 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFTTrainer 구성\n",
    "\n",
    "`SFTTrainer`는 훈련 프로세스를 제어하는 다양한 매개변수로 구성됩니다. 여기에는 훈련 단계 수, 배치 크기, 학습률 및 평가 전략이 포함됩니다. 특정 요구 사항 및 계산 리소스에 따라 이러한 매개변수를 조정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTTrainer 구성\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # 데이터 세트 크기 및 원하는 훈련 기간에 따라 조정합니다.\n",
    "    per_device_train_batch_size=4,  # GPU 메모리 용량에 따라 설정합니다.\n",
    "    learning_rate=5e-5,  # 미세 조정을 위한 일반적인 시작점입니다.\n",
    "    logging_steps=10,  # 훈련 메트릭 로깅 빈도입니다.\n",
    "    save_steps=100,  # 모델 체크포인트 저장 빈도입니다.\n",
    "    evaluation_strategy=\"steps\",  # 정기적으로 모델을 평가합니다.\n",
    "    eval_steps=50,  # 평가 빈도입니다.\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # 혼합 정밀도 훈련에 MPS를 사용합니다.\n",
    "    hub_model_id=finetune_name,  # 모델에 고유한 이름을 설정합니다.\n",
    ")\n",
    "\n",
    "# SFTTrainer 초기화\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=ds[\"test\"],\n",
    ")\n",
    "\n",
    "# TODO: 🦁 🐕 선택한 데이터 세트에 SFTTrainer 매개변수를 맞춥니다. 예를 들어 `bigcode/the-stack-smol` 데이터 세트를 사용하는 경우 `content` 열을 선택해야 합니다.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 훈련\n",
    "\n",
    "트레이너가 구성되었으므로 이제 모델 훈련을 진행할 수 있습니다. 훈련 프로세스에는 데이터 세트를 반복하고, 손실을 계산하고, 이 손실을 최소화하도록 모델의 매개변수를 업데이트하는 과정이 포함됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model(f\"./{finetune_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(tags=finetune_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>보너스 연습: 미세 조정된 모델로 생성</h2>\n",
    "    <p>🐕 기본 예제와 마찬가지로 미세 조정된 모델을 사용하여 응답을 생성합니다.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일한 프롬프트에서 미세 조정된 모델 테스트\n",
    "\n",
    "# 훈련 전에 기본 모델을 테스트합니다.\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# 템플릿으로 형식 지정\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# 응답 생성\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# TODO: 기본 예제와 마찬가지로 미세 조정된 모델을 사용하여 응답을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💐 완료했습니다!\n",
    "\n",
    "이 노트북은 `SFTTrainer`를 사용하여 `HuggingFaceTB/SmolLM2-135M` 모델을 미세 조정하는 단계별 가이드를 제공했습니다. 이러한 단계를 따르면 특정 작업을 보다 효과적으로 수행하도록 모델을 조정할 수 있습니다. 이 과정을 계속 진행하려면 다음 단계를 시도해 볼 수 있습니다.\n",
    "\n",
    "- 더 어려운 난이도로 이 노트북을 시도해 보십시오.\n",
    "- 동료의 PR을 검토하십시오.\n",
    "- 문제 또는 PR을 통해 과정 자료를 개선하십시오."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
