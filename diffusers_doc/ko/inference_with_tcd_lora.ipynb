{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 궤적 일관성 증류-LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "궤적 일관성 증류(TCD)를 사용하면 모델이 더 적은 단계로 더 높은 품질과 더 자세한 이미지를 생성할 수 있습니다. 또한 증류 과정에서 효과적인 오류 완화 덕분에 TCD는 많은 추론 단계 조건에서도 우수한 성능을 보여줍니다.\n",
    "\n",
    "TCD의 주요 장점은 다음과 같습니다.\n",
    "\n",
    "- 교사보다 우수: TCD는 작고 큰 추론 단계 모두에서 우수한 생성 품질을 보여주며 Stable Diffusion XL(SDXL)을 사용한 [DPM-Solver++(2S)](https://huggingface.co/docs/diffusers/main/en/using-diffusers/../../api/schedulers/multistep_dpm_solver)의 성능을 능가합니다. TCD 학습 중에는 추가 판별기 또는 LPIPS 감독이 포함되지 않습니다.\n",
    "\n",
    "- 유연한 추론 단계: TCD 샘플링의 추론 단계는 이미지 품질에 부정적인 영향을 미치지 않고 자유롭게 조정할 수 있습니다.\n",
    "\n",
    "- 세부 수준 자유롭게 변경: 추론 중에 단일 하이퍼파라미터인 *감마*를 사용하여 이미지의 세부 수준을 조정할 수 있습니다.\n",
    "\n",
    "> [!TIP]\n",
    "> TCD에 대한 자세한 기술 내용은 [논문](https://huggingface.co/papers/2402.19159) 또는 공식 [프로젝트 페이지](https://mhh0318.github.io/tcd/)를 참조하십시오.\n",
    "\n",
    "SDXL과 같은 대규모 모델의 경우 TCD는 [LoRA](https://huggingface.co/docs/peft/conceptual_guides/adapter#low-rank-adaptation-lora)를 사용하여 메모리 사용량을 줄여 학습합니다. 이는 동일한 기본 모델을 공유하는 한 추가 학습 없이 서로 다른 미세 조정 모델 간에 LoRA를 재사용할 수 있기 때문에 유용합니다.\n",
    "\n",
    "\n",
    "\n",
    "이 가이드에서는 텍스트-이미지 및 인페인팅과 같은 다양한 작업에 대해 TCD-LoRA를 사용하여 추론을 수행하는 방법과 TCD-LoRA를 다른 어댑터와 쉽게 결합하는 방법을 보여줍니다. 시작하려면 아래 표에서 지원되는 기본 모델과 해당 TCD-LoRA 체크포인트 중 하나를 선택하십시오.\n",
    "\n",
    "| 기본 모델                                                                                      | TCD-LoRA 체크포인트                                            |\n",
    "|-------------------------------------------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| [stable-diffusion-v1-5](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5)                  | [TCD-SD15](https://huggingface.co/h1t/TCD-SD15-LoRA)           |\n",
    "| [stable-diffusion-2-1-base](https://huggingface.co/stabilityai/stable-diffusion-2-1-base)       | [TCD-SD21-base](https://huggingface.co/h1t/TCD-SD21-base-LoRA) |\n",
    "| [stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | [TCD-SDXL](https://huggingface.co/h1t/TCD-SDXL-LoRA)           |\n",
    "\n",
    "\n",
    "더 나은 LoRA 지원을 위해 [PEFT](https://github.com/huggingface/peft)가 설치되어 있는지 확인하십시오.\n",
    "\n",
    "```bash\n",
    "pip install -U peft\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일반 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 가이드에서는 [StableDiffusionXLPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline)과 [TCDScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/tcd#diffusers.TCDScheduler)를 사용합니다. [load_lora_weights()](https://huggingface.co/docs/diffusers/main/en/api/loaders/lora#diffusers.loaders.StableDiffusionLoraLoaderMixin.load_lora_weights) 메서드를 사용하여 SDXL 호환 TCD-LoRA 가중치를 로드합니다.\n",
    "\n",
    "TCD-LoRA 추론 시 염두에 두어야 할 몇 가지 팁은 다음과 같습니다.\n",
    "\n",
    "- `num_inference_steps`를 4에서 50 사이로 유지합니다.\n",
    "- `eta`(각 단계에서 확률성을 제어하는 데 사용됨)를 0에서 1 사이로 설정합니다. 추론 단계 수를 늘릴 때 더 높은 `eta`를 사용해야 하지만, [TCDScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/tcd#diffusers.TCDScheduler)에서 `eta`가 클수록 이미지가 흐릿해지는 단점이 있습니다. 좋은 결과를 얻으려면 0.3 값을 권장합니다.\n",
    "\n",
    "<hfoptions id=\"tasks\">\n",
    "<hfoption id=\"text-to-image\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, TCDScheduler\n",
    "\n",
    "device = \"cuda\"\n",
    "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "prompt = \"Painting of the orange cat Otto von Garfield, Count of Bismarck-Schönhausen, Duke of Lauenburg, Minister-President of Prussia. Depicted wearing a Prussian Pickelhaube and eating his favorite meal - lasagna.\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/jabir-zheng/TCD/raw/main/assets/demo_image.png)\n",
    "\n",
    "</hfoption>\n",
    "\n",
    "<hfoption id=\"inpainting\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForInpainting, TCDScheduler\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "device = \"cuda\"\n",
    "base_model_id = \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "pipe = AutoPipelineForInpainting.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "\n",
    "init_image = load_image(img_url).resize((1024, 1024))\n",
    "mask_image = load_image(mask_url).resize((1024, 1024))\n",
    "\n",
    "prompt = \"a tiger sitting on a park bench\"\n",
    "\n",
    "image = pipe(\n",
    "  prompt=prompt,\n",
    "  image=init_image,\n",
    "  mask_image=mask_image,\n",
    "  num_inference_steps=8,\n",
    "  guidance_scale=0,\n",
    "  eta=0.3,\n",
    "  strength=0.99,  # `strength`는 1.0 미만으로 사용해야 합니다.\n",
    "  generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]\n",
    "\n",
    "grid_image = make_image_grid([init_image, mask_image, image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/jabir-zheng/TCD/raw/main/assets/inpainting_tcd.png)\n",
    "\n",
    "\n",
    "</hfoption>\n",
    "</hfoptions>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 커뮤니티 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCD-LoRA는 또한 많은 커뮤니티에서 미세 조정한 모델 및 플러그인과 함께 작동합니다. 예를 들어, 애니메이션 이미지를 생성하기 위해 커뮤니티에서 미세 조정한 SDXL 버전인 [animagine-xl-3.0](https://huggingface.co/cagliostrolab/animagine-xl-3.0) 체크포인트를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, TCDScheduler\n",
    "\n",
    "device = \"cuda\"\n",
    "base_model_id = \"cagliostrolab/animagine-xl-3.0\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "prompt = \"A man, clad in a meticulously tailored military uniform, stands with unwavering resolve. The uniform boasts intricate details, and his eyes gleam with determination. Strands of vibrant, windswept hair peek out from beneath the brim of his cap.\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=8,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/jabir-zheng/TCD/raw/main/assets/animagine_xl.png)\n",
    "\n",
    "TCD-LoRA는 다른 스타일로 학습된 다른 LoRA도 지원합니다. 예를 들어, [TheLastBen/Papercut_SDXL](https://huggingface.co/TheLastBen/Papercut_SDXL) LoRA를 로드하고 `~loaders.UNet2DConditionLoadersMixin.set_adapters` 메서드를 사용하여 TCD-LoRA와 융합합니다.\n",
    "\n",
    "> [!TIP]\n",
    "> 효율적인 병합 방법에 대한 자세한 내용은 [LoRA 병합](https://huggingface.co/docs/diffusers/main/en/using-diffusers/merge_loras) 가이드를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from scheduling_tcd import TCDScheduler\n",
    "\n",
    "device = \"cuda\"\n",
    "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "styled_lora_id = \"TheLastBen/Papercut_SDXL\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(device)\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id, adapter_name=\"tcd\")\n",
    "pipe.load_lora_weights(styled_lora_id, adapter_name=\"style\")\n",
    "pipe.set_adapters([\"tcd\", \"style\"], adapter_weights=[1.0, 1.0])\n",
    "\n",
    "prompt = \"papercut of a winter mountain, snow\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/jabir-zheng/TCD/raw/main/assets/styled_lora.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어댑터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCD-LoRA는 매우 다재다능하며 ControlNet, IP-Adapter 및 AnimateDiff와 같은 다른 어댑터 유형과 결합할 수 있습니다.\n",
    "\n",
    "<hfoptions id=\"adapters\">\n",
    "<hfoption id=\"ControlNet\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 깊이 제어망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from scheduling_tcd import TCDScheduler\n",
    "\n",
    "device = \"cuda\"\n",
    "depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(device)\n",
    "feature_extractor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "def get_depth_map(image):\n",
    "    image = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    with torch.no_grad(), torch.autocast(device):\n",
    "        depth_map = depth_estimator(image).predicted_depth\n",
    "\n",
    "    depth_map = torch.nn.functional.interpolate(\n",
    "        depth_map.unsqueeze(1),\n",
    "        size=(1024, 1024),\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "    image = torch.cat([depth_map] * 3, dim=1)\n",
    "\n",
    "    image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
    "    image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "controlnet_id = \"diffusers/controlnet-depth-sdxl-1.0\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    controlnet_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    base_model_id,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "prompt = \"stormtrooper lecture, photorealistic\"\n",
    "\n",
    "image = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-depth/resolve/main/images/stormtrooper.png\")\n",
    "depth_image = get_depth_map(image)\n",
    "\n",
    "controlnet_conditioning_scale = 0.5  # 좋은 일반화를 위해 권장됨\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=depth_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "    generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]\n",
    "\n",
    "grid_image = make_image_grid([depth_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/jabir-zheng/TCD/raw/main/assets/controlnet_depth_tcd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캐니 컨트롤넷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from scheduling_tcd import TCDScheduler\n",
    "\n",
    "device = \"cuda\"\n",
    "base_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "controlnet_id = \"diffusers/controlnet-canny-sdxl-1.0\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    controlnet_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    base_model_id,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "prompt = \"ultrarealistic shot of a furry blue bird\"\n",
    "\n",
    "canny_image = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/bird_canny.png\")\n",
    "\n",
    "controlnet_conditioning_scale = 0.5  # 좋은 일반화를 위해 권장됨\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image=canny_image,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "    generator=torch.Generator(device=device).manual_seed(0),\n",
    ").images[0]\n",
    "\n",
    "grid_image = make_image_grid([canny_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/jabir-zheng/TCD/raw/main/assets/controlnet_canny_tcd.png)\n",
    "\n",
    "<Tip>\n",
    "이 예제의 추론 매개변수는 모든 예제에 적용되지 않을 수 있으므로 `num_inference_steps`, `guidance_scale`, `controlnet_conditioning_scale` 및 `cross_attention_kwargs` 매개변수에 대해 다른 값을 시도하고 가장 적합한 값을 선택하는 것이 좋습니다.\n",
    "</Tip>\n",
    "\n",
    "</hfoption>\n",
    "<hfoption id=\"IP-Adapter\">\n",
    "\n",
    "이 예제에서는 [IP-Adapter](https://github.com/tencent-ailab/IP-Adapter/tree/main) 및 SDXL과 함께 TCD-LoRA를 사용하는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "from ip_adapter import IPAdapterXL\n",
    "from scheduling_tcd import TCDScheduler\n",
    "\n",
    "device = \"cuda\"\n",
    "base_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "image_encoder_path = \"sdxl_models/image_encoder\"\n",
    "ip_ckpt = \"sdxl_models/ip-adapter_sdxl.bin\"\n",
    "tcd_lora_id = \"h1t/TCD-SDXL-LoRA\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "pipe.load_lora_weights(tcd_lora_id)\n",
    "pipe.fuse_lora()\n",
    "\n",
    "ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "ref_image = load_image(\"https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/images/woman.png\").resize((512, 512))\n",
    "\n",
    "prompt = \"best quality, high quality, wearing sunglasses\"\n",
    "\n",
    "image = ip_model.generate(\n",
    "    pil_image=ref_image,\n",
    "    prompt=prompt,\n",
    "    scale=0.5,\n",
    "    num_samples=1,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0,\n",
    "    eta=0.3,\n",
    "    seed=0,\n",
    ")[0]\n",
    "\n",
    "grid_image = make_image_grid([ref_image, image], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/jabir-zheng/TCD/raw/main/assets/ip_adapter.png)\n",
    "\n",
    "\n",
    "\n",
    "</hfoption>\n",
    "<hfoption id=\"AnimateDiff\">\n",
    "\n",
    "`AnimateDiff`를 사용하면 스테이블 디퓨전 모델을 사용하여 이미지를 애니메이션으로 만들 수 있습니다. TCD-LoRA는 이미지 품질을 저하시키지 않고 프로세스를 크게 가속화할 수 있습니다. TCD-LoRA와 AnimateDiff를 사용한 애니메이션 품질은 더욱 선명한 결과를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler\n",
    "from scheduling_tcd import TCDScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "\n",
    "adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5\")\n",
    "pipe = AnimateDiffPipeline.from_pretrained(\n",
    "    \"frankjoshua/toonyou_beta6\",\n",
    "    motion_adapter=adapter,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# TCDScheduler 설정\n",
    "pipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# TCD LoRA 로드\n",
    "pipe.load_lora_weights(\"h1t/TCD-SD15-LoRA\", adapter_name=\"tcd\")\n",
    "pipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", weight_name=\"diffusion_pytorch_model.safetensors\", adapter_name=\"motion-lora\")\n",
    "\n",
    "pipe.set_adapters([\"tcd\", \"motion-lora\"], adapter_weights=[1.0, 1.2])\n",
    "\n",
    "prompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\n",
    "generator = torch.manual_seed(0)\n",
    "frames = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=5,\n",
    "    guidance_scale=0,\n",
    "    cross_attention_kwargs={\"scale\": 1},\n",
    "    num_frames=24,\n",
    "    eta=0.3,\n",
    "    generator=generator\n",
    ").frames[0]\n",
    "export_to_gif(frames, \"animation.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/jabir-zheng/TCD/raw/main/assets/animation_example.gif)\n",
    "\n",
    "</hfoption>\n",
    "</hfoptions>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
