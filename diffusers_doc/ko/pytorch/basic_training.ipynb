{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "license: apache-2.0\n",
    "canonical_url: https://huggingface.co/docs/diffusers/training/basic_training\n",
    "---\n",
    "\n",
    "# 기본 훈련 예제\n",
    "\n",
    "이 튜토리얼은 [`DiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline)을 훈련시키는 간단한 예제를 보여줍니다. 이 예제는 [PyTorch](https://pytorch.org/)를 사용하여 작성되었지만, [Jax/Flax](https://github.com/huggingface/diffusers/tree/main/examples/basic_text_image_fine_tune)에서도 동일한 작업을 수행할 수 있습니다. 전체 훈련 스크립트는 [여기](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/train_unconditional.py)에서 찾을 수 있습니다.\n",
    "\n",
    "훈련을 시작하기 전에 모든 필수 라이브러리가 설치되어 있는지 확인하십시오:\n",
    "\n",
    "```bash\n",
    "pip install diffusers accelerate transformers ftfy\n",
    "```\n",
    "\n",
    "이 튜토리얼에서는 [Smithsonian Butterfly Collection](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) 데이터 세트에서 나비 이미지를 생성하도록 모델을 미세 조정합니다. 이 데이터 세트에는 1000개의 나비 이미지가 포함되어 있으며, 이는 모델이 새로운 것을 학습하기에 충분히 작습니다.\n",
    "\n",
    "시작하려면 [Hugging Face Hub](https://huggingface.co/models)에서 사전 훈련된 모델을 로드합니다. 이 튜토리얼에서는 [`google/ddpm-cat-256`](https://huggingface.co/google/ddpm-cat-256) 체크포인트를 사용합니다. 이 체크포인트는 고양이 이미지에서 조건 없이 훈련되었습니다. 이 모델을 사용하여 나비를 생성하는 방법을 알아보겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 세트 구성\n",
    "\n",
    "훈련을 위해 [🤗 Datasets](https://huggingface.co/docs/datasets/) 라이브러리를 사용하여 데이터 세트를 다운로드하고 전처리합니다. 다음과 같이 간단히 데이터 세트를 로드할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로, 이미지를 전처리합니다. 이미지 크기를 모델의 예상 입력과 일치하도록 조정하고, `ToTensor` 변환을 적용하여 이미지를 PyTorch 텐서로 변환합니다. 마지막으로, 이미지를 정규화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "image_size = 256\n",
    "batch_size = 16\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 세트에 이러한 변환을 적용하기 위해 `set_transform` 함수를 사용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 다음 데이터 세트를 `DataLoader`로 래핑하여 훈련 중에 데이터를 반복할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성\n",
    "\n",
    "다음으로, [`UNet2DModel`](https://huggingface.co/docs/diffusers/main/en/api/models#diffusers.UNet2DModel)을 로드하고 구성합니다. 모델이 학습할 이미지의 크기와 일치하도록 `sample_size`를 설정해야 합니다. `layers_per_block`을 설정하여 UNet 아키텍처를 변경할 수도 있습니다. 이 매개변수는 각 다운샘플링 및 업샘플링 블록의 ResNet 블록 수를 정의합니다. 이 튜토리얼에서는 기본값을 유지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model_name = \"google/ddpm-cat-256\"\n",
    "model = UNet2DModel.from_pretrained(\n",
    "    model_name, \n",
    "    use_safetensors=True,\n",
    "    # num_train_timesteps=1000,\n",
    "    # beta_start=0.0001,\n",
    "    # beta_end=0.02,\n",
    "    # beta_schedule=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 훈련시키는 동안 훈련 진행 상황을 모니터링하기 위해 모델의 샘플 출력을 주기적으로 저장하는 것이 좋습니다. `DDPMPipeline`은 이미지 생성을 위한 편리한 방법이며, 훈련 루프에서 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "image_pipe = DDPMPipeline(unet=model, scheduler=noise_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스케줄러 구성\n",
    "\n",
    "확산 모델은 노이즈 스케줄러를 사용하여 점진적으로 이미지에 노이즈를 추가하고 역으로 노이즈를 제거합니다. [`DDPMScheduler`](https://huggingface.co/docs/diffusers/main/en/api/schedulers#diffusers.DDPMScheduler)를 사용하여 훈련 중에 노이즈 제거 프로세스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련\n",
    "\n",
    "이제 거의 모든 준비가 되었습니다! 마지막 단계는 옵티마이저를 설정하고 훈련 루프를 지정하는 것입니다. 이 튜토리얼에서는 AdamW 옵티마이저를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 훈련 루프를 작성할 수 있습니다. 루프는 데이터를 반복하고, 각 단계에서 모델은 노이즈가 있는 이미지를 예측하려고 시도합니다. 그런 다음 손실 함수는 모델의 예측과 실제 값(이 예에서는 평균 제곱 오차) 간의 차이를 계산합니다. 그런 다음 손실을 사용하여 모델의 기울기를 업데이트합니다.\n",
    "\n",
    "훈련 루프는 [🤗 Accelerate](https://huggingface.co/docs/accelerate)를 사용하여 분산 환경에서 훈련을 처리합니다. `Accelerator` 객체를 인스턴스화한 다음 훈련 객체를 `prepare` 함수로 보냅니다. 또한 Accelerate는 `gradient_accumulation_steps` 인수를 지원하여 효과적인 배치 크기를 늘려 그래디언트 누적을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 훈련 루프를 실행할 수 있습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "\n",
    "output_dir = \"ddpm-butterflies-256\"\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction\n",
    "        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward(loss)\n",
    "\n",
    "        # Update the model parameters with the optimizer\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # After each epoch, log a few images from the pipeline to see how training is going\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler) # TODO: make a pipeline from the model\n",
    "        images = pipeline(\n",
    "            batch_size = 4, \n",
    "            generator=torch.manual_seed(0),\n",
    "        ).images\n",
    "        # make_image_grid(images, rows=2, cols=2).save(f\"{output_dir}/{epoch:04d}.png\")\n",
    "        print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련이 완료되면 훈련된 모델을 [Hugging Face Hub](https://huggingface.co/models)에 업로드하여 모든 사람이 사용할 수 있도록 합니다. 모델을 업로드하려면 Hugging Face 계정에 로그인해야 합니다. 아직 계정이 없다면 [여기](https://huggingface.co/join)에서 계정을 만드십시오.\n",
    "\n",
    "모델을 Hub에 푸시하기 위한 `push_to_hub` 함수를 만듭니다. 이 함수는 모델, 체크포인트 이름 및 커밋 메시지와 같은 몇 가지 매개변수를 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, ModelCard, create_repo, get_full_repo_name\n",
    "\n",
    "def save_model_card(repo_id: str, images=None, base_model=str, dataset_name=str, repo_folder=None):\n",
    "    img_str = \"\"\n",
    "    if images is not None:\n",
    "        for i, ns_image in enumerate(images):\n",
    "            # logger.info(f\"Saving sample {i}.png\")\n",
    "            ns_image.save(os.path.join(repo_folder, f\"sample_{i}.png\"))\n",
    "            img_str += f\"![img{i}](./sample_{i}.png)\"\n",
    "    yaml = f\"\"\"\n",
    "---    \n",
    "base_model: {base_model}\n",
    "instance_prompt: photograph of a [OBJECT] butterfly\n",
    "tags:\n",
    "- diffusers\n",
    "- unconditional-image-generation\n",
    "- diffusion-models-class\n",
    "license: mit\n",
    "inference: false\n",
    "--- \n",
    "\"\"\"\n",
    "    model_card = f\"\"\"\n",
    "# DDPM - {repo_id}\n",
    "\n",
    "This is a diffusion model trained on the {dataset_name} dataset.      \n",
    "\n",
    "{img_str}\n",
    "\"\"\"\n",
    "    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n",
    "        f.write(yaml + model_card)\n",
    "\n",
    "\n",
    "def upload_folder(repo_id, folder_path, path_in_repo):\n",
    "    # 업로드할 파일 경로 목록 만들기\n",
    "    files = [\n",
    "        os.path.join(path, file)\n",
    "        for path, _, files in os.walk(folder_path)\n",
    "        for file in files\n",
    "    ]\n",
    "    # 각 파일을 repo에 업로드\n",
    "    # logger.info(f\"업로드 중인 파일 수: {len(files)}\")\n",
    "    for file_path in files:\n",
    "        # repo 내 파일 경로 만들기\n",
    "        destination_path = os.path.join(\n",
    "            path_in_repo, os.path.relpath(file_path, folder_path)\n",
    "        )\n",
    "        # logger.info(f\"'{file_path}' 업로드 중 -> '{destination_path}'\")\n",
    "        # 파일 업로드\n",
    "        try:\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=file_path,\n",
    "                path_in_repo=destination_path,\n",
    "                repo_id=repo_id,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # logger.error(f\"파일 업로드 중 오류 발생 {file_path}: {e}\")\n",
    "            pass\n",
    "\n",
    "model_name = \"ddpm-butterflies-10k_epochs\"\n",
    "hub_model_id = f\"sasha/{model_name}\"\n",
    "\n",
    "pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "pipeline.save_pretrained(output_dir)\n",
    "\n",
    "repo_id = create_repo(hub_model_id, exist_ok=True, private=True).repo_id\n",
    "\n",
    "with open(os.path.join(output_dir, \"_config.json\"), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "save_model_card(\n",
    "    repo_id,\n",
    "    base_model=model_name,\n",
    "    images=images,\n",
    "    repo_folder=output_dir,\n",
    ")\n",
    "upload_folder(\n",
    "    repo_id,\n",
    "    output_dir,\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 훈련된 모델을 Hub에 푸시했으므로 이제 누구나 [`DiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) 클래스로 로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(hub_model_id, use_safetensors=True)\n",
    "image = pipeline(num_inference_steps=250).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하합니다! 이제 확산 모델을 훈련하고 Hub에 업로드하는 방법을 알게 되었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
