{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "license: apache-2.0\n",
    "canonical_url: https://huggingface.co/docs/diffusers/training/basic_training\n",
    "---\n",
    "\n",
    "# ê¸°ë³¸ í›ˆë ¨ ì˜ˆì œ\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì€ [`DiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline)ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì˜ˆì œëŠ” [PyTorch](https://pytorch.org/)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì„±ë˜ì—ˆì§€ë§Œ, [Jax/Flax](https://github.com/huggingface/diffusers/tree/main/examples/basic_text_image_fine_tune)ì—ì„œë„ ë™ì¼í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ì²´ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ëŠ” [ì—¬ê¸°](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/train_unconditional.py)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì— ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤:\n",
    "\n",
    "```bash\n",
    "pip install diffusers accelerate transformers ftfy\n",
    "```\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” [Smithsonian Butterfly Collection](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë‚˜ë¹„ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë„ë¡ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤. ì´ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” 1000ê°œì˜ ë‚˜ë¹„ ì´ë¯¸ì§€ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì´ ìƒˆë¡œìš´ ê²ƒì„ í•™ìŠµí•˜ê¸°ì— ì¶©ë¶„íˆ ì‘ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‹œì‘í•˜ë ¤ë©´ [Hugging Face Hub](https://huggingface.co/models)ì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” [`google/ddpm-cat-256`](https://huggingface.co/google/ddpm-cat-256) ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ì²´í¬í¬ì¸íŠ¸ëŠ” ê³ ì–‘ì´ ì´ë¯¸ì§€ì—ì„œ ì¡°ê±´ ì—†ì´ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‚˜ë¹„ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„° ì„¸íŠ¸ êµ¬ì„±\n",
    "\n",
    "í›ˆë ¨ì„ ìœ„í•´ [ğŸ¤— Datasets](https://huggingface.co/docs/datasets/) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨íˆ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒìœ¼ë¡œ, ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ëª¨ë¸ì˜ ì˜ˆìƒ ì…ë ¥ê³¼ ì¼ì¹˜í•˜ë„ë¡ ì¡°ì •í•˜ê³ , `ToTensor` ë³€í™˜ì„ ì ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ë¯¸ì§€ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "image_size = 256\n",
    "batch_size = 16\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì„¸íŠ¸ì— ì´ëŸ¬í•œ ë³€í™˜ì„ ì ìš©í•˜ê¸° ìœ„í•´ `set_transform` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ° ë‹¤ìŒ ë°ì´í„° ì„¸íŠ¸ë¥¼ `DataLoader`ë¡œ ë˜í•‘í•˜ì—¬ í›ˆë ¨ ì¤‘ì— ë°ì´í„°ë¥¼ ë°˜ë³µí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ êµ¬ì„±\n",
    "\n",
    "ë‹¤ìŒìœ¼ë¡œ, [`UNet2DModel`](https://huggingface.co/docs/diffusers/main/en/api/models#diffusers.UNet2DModel)ì„ ë¡œë“œí•˜ê³  êµ¬ì„±í•©ë‹ˆë‹¤. ëª¨ë¸ì´ í•™ìŠµí•  ì´ë¯¸ì§€ì˜ í¬ê¸°ì™€ ì¼ì¹˜í•˜ë„ë¡ `sample_size`ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. `layers_per_block`ì„ ì„¤ì •í•˜ì—¬ UNet ì•„í‚¤í…ì²˜ë¥¼ ë³€ê²½í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ê° ë‹¤ìš´ìƒ˜í”Œë§ ë° ì—…ìƒ˜í”Œë§ ë¸”ë¡ì˜ ResNet ë¸”ë¡ ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ê¸°ë³¸ê°’ì„ ìœ ì§€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model_name = \"google/ddpm-cat-256\"\n",
    "model = UNet2DModel.from_pretrained(\n",
    "    model_name, \n",
    "    use_safetensors=True,\n",
    "    # num_train_timesteps=1000,\n",
    "    # beta_start=0.0001,\n",
    "    # beta_end=0.02,\n",
    "    # beta_schedule=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë™ì•ˆ í›ˆë ¨ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ ìƒ˜í”Œ ì¶œë ¥ì„ ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. `DDPMPipeline`ì€ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ í¸ë¦¬í•œ ë°©ë²•ì´ë©°, í›ˆë ¨ ë£¨í”„ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "image_pipe = DDPMPipeline(unet=model, scheduler=noise_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬ì„±\n",
    "\n",
    "í™•ì‚° ëª¨ë¸ì€ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ê³  ì—­ìœ¼ë¡œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•©ë‹ˆë‹¤. [`DDPMScheduler`](https://huggingface.co/docs/diffusers/main/en/api/schedulers#diffusers.DDPMScheduler)ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ì¤‘ì— ë…¸ì´ì¦ˆ ì œê±° í”„ë¡œì„¸ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í›ˆë ¨\n",
    "\n",
    "ì´ì œ ê±°ì˜ ëª¨ë“  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •í•˜ê³  í›ˆë ¨ ë£¨í”„ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ í›ˆë ¨ ë£¨í”„ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë£¨í”„ëŠ” ë°ì´í„°ë¥¼ ë°˜ë³µí•˜ê³ , ê° ë‹¨ê³„ì—ì„œ ëª¨ë¸ì€ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì†ì‹¤ í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê°’(ì´ ì˜ˆì—ì„œëŠ” í‰ê·  ì œê³± ì˜¤ì°¨) ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì†ì‹¤ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ê¸°ìš¸ê¸°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "í›ˆë ¨ ë£¨í”„ëŠ” [ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì‚° í™˜ê²½ì—ì„œ í›ˆë ¨ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. `Accelerator` ê°ì²´ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•œ ë‹¤ìŒ í›ˆë ¨ ê°ì²´ë¥¼ `prepare` í•¨ìˆ˜ë¡œ ë³´ëƒ…ë‹ˆë‹¤. ë˜í•œ AccelerateëŠ” `gradient_accumulation_steps` ì¸ìˆ˜ë¥¼ ì§€ì›í•˜ì—¬ íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë ¤ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ í›ˆë ¨ ë£¨í”„ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "\n",
    "output_dir = \"ddpm-butterflies-256\"\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        clean_images = batch[\"images\"].to(device)\n",
    "        # Sample noise to add to the images\n",
    "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "        bs = clean_images.shape[0]\n",
    "\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device\n",
    "        ).long()\n",
    "\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction\n",
    "        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward(loss)\n",
    "\n",
    "        # Update the model parameters with the optimizer\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # After each epoch, log a few images from the pipeline to see how training is going\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler) # TODO: make a pipeline from the model\n",
    "        images = pipeline(\n",
    "            batch_size = 4, \n",
    "            generator=torch.manual_seed(0),\n",
    "        ).images\n",
    "        # make_image_grid(images, rows=2, cols=2).save(f\"{output_dir}/{epoch:04d}.png\")\n",
    "        print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ í›ˆë ¨ëœ ëª¨ë¸ì„ [Hugging Face Hub](https://huggingface.co/models)ì— ì—…ë¡œë“œí•˜ì—¬ ëª¨ë“  ì‚¬ëŒì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ì•„ì§ ê³„ì •ì´ ì—†ë‹¤ë©´ [ì—¬ê¸°](https://huggingface.co/join)ì—ì„œ ê³„ì •ì„ ë§Œë“œì‹­ì‹œì˜¤.\n",
    "\n",
    "ëª¨ë¸ì„ Hubì— í‘¸ì‹œí•˜ê¸° ìœ„í•œ `push_to_hub` í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸, ì²´í¬í¬ì¸íŠ¸ ì´ë¦„ ë° ì»¤ë°‹ ë©”ì‹œì§€ì™€ ê°™ì€ ëª‡ ê°€ì§€ ë§¤ê°œë³€ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, ModelCard, create_repo, get_full_repo_name\n",
    "\n",
    "def save_model_card(repo_id: str, images=None, base_model=str, dataset_name=str, repo_folder=None):\n",
    "    img_str = \"\"\n",
    "    if images is not None:\n",
    "        for i, ns_image in enumerate(images):\n",
    "            # logger.info(f\"Saving sample {i}.png\")\n",
    "            ns_image.save(os.path.join(repo_folder, f\"sample_{i}.png\"))\n",
    "            img_str += f\"![img{i}](./sample_{i}.png)\"\n",
    "    yaml = f\"\"\"\n",
    "---    \n",
    "base_model: {base_model}\n",
    "instance_prompt: photograph of a [OBJECT] butterfly\n",
    "tags:\n",
    "- diffusers\n",
    "- unconditional-image-generation\n",
    "- diffusion-models-class\n",
    "license: mit\n",
    "inference: false\n",
    "--- \n",
    "\"\"\"\n",
    "    model_card = f\"\"\"\n",
    "# DDPM - {repo_id}\n",
    "\n",
    "This is a diffusion model trained on the {dataset_name} dataset.      \n",
    "\n",
    "{img_str}\n",
    "\"\"\"\n",
    "    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n",
    "        f.write(yaml + model_card)\n",
    "\n",
    "\n",
    "def upload_folder(repo_id, folder_path, path_in_repo):\n",
    "    # ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ ëª©ë¡ ë§Œë“¤ê¸°\n",
    "    files = [\n",
    "        os.path.join(path, file)\n",
    "        for path, _, files in os.walk(folder_path)\n",
    "        for file in files\n",
    "    ]\n",
    "    # ê° íŒŒì¼ì„ repoì— ì—…ë¡œë“œ\n",
    "    # logger.info(f\"ì—…ë¡œë“œ ì¤‘ì¸ íŒŒì¼ ìˆ˜: {len(files)}\")\n",
    "    for file_path in files:\n",
    "        # repo ë‚´ íŒŒì¼ ê²½ë¡œ ë§Œë“¤ê¸°\n",
    "        destination_path = os.path.join(\n",
    "            path_in_repo, os.path.relpath(file_path, folder_path)\n",
    "        )\n",
    "        # logger.info(f\"'{file_path}' ì—…ë¡œë“œ ì¤‘ -> '{destination_path}'\")\n",
    "        # íŒŒì¼ ì—…ë¡œë“œ\n",
    "        try:\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=file_path,\n",
    "                path_in_repo=destination_path,\n",
    "                repo_id=repo_id,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # logger.error(f\"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ {file_path}: {e}\")\n",
    "            pass\n",
    "\n",
    "model_name = \"ddpm-butterflies-10k_epochs\"\n",
    "hub_model_id = f\"sasha/{model_name}\"\n",
    "\n",
    "pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "pipeline.save_pretrained(output_dir)\n",
    "\n",
    "repo_id = create_repo(hub_model_id, exist_ok=True, private=True).repo_id\n",
    "\n",
    "with open(os.path.join(output_dir, \"_config.json\"), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "save_model_card(\n",
    "    repo_id,\n",
    "    base_model=model_name,\n",
    "    images=images,\n",
    "    repo_folder=output_dir,\n",
    ")\n",
    "upload_folder(\n",
    "    repo_id,\n",
    "    output_dir,\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ í›ˆë ¨ëœ ëª¨ë¸ì„ Hubì— í‘¸ì‹œí–ˆìœ¼ë¯€ë¡œ ì´ì œ ëˆ„êµ¬ë‚˜ [`DiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline) í´ë˜ìŠ¤ë¡œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(hub_model_id, use_safetensors=True)\n",
    "image = pipeline(num_inference_steps=250).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶•í•˜í•©ë‹ˆë‹¤! ì´ì œ í™•ì‚° ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  Hubì— ì—…ë¡œë“œí•˜ëŠ” ë°©ë²•ì„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
