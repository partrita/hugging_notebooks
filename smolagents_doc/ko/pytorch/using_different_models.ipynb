{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치\n",
    "! pip install smolagents\n",
    "# 마지막 릴리스 대신 소스에서 설치하려면 위 명령을 주석 처리하고 다음 명령의 주석을 해제하십시오.\n",
    "# ! pip install git+https://github.com/huggingface/smolagents.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 모델 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`smolagents`는 다양한 공급자의 다양한 언어 모델을 사용할 수 있는 유연한 프레임워크를 제공합니다.\n",
    "이 가이드에서는 에이전트와 함께 다양한 모델 유형을 사용하는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 가능한 모델 유형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`smolagents`는 기본적으로 여러 모델 유형을 지원합니다.\n",
    "1. [InferenceClientModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.InferenceClientModel): Hugging Face의 추론 API를 사용하여 모델에 액세스합니다.\n",
    "2. [TransformersModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.TransformersModel): Transformers 라이브러리를 사용하여 로컬에서 모델을 실행합니다.\n",
    "3. [VLLMModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.VLLMModel): 최적화된 서빙으로 빠른 추론을 위해 vLLM을 사용합니다.\n",
    "4. [MLXModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.MLXModel): MLX를 사용하여 Apple Silicon 장치에 최적화되었습니다.\n",
    "5. [LiteLLMModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.LiteLLMModel): LiteLLM을 통해 수백 개의 LLM에 대한 액세스를 제공합니다.\n",
    "6. [LiteLLMRouterModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.LiteLLMRouterModel): 여러 모델 간에 요청을 분산합니다.\n",
    "7. [OpenAIServerModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.OpenAIServerModel): OpenAI 호환 API를 구현하는 모든 공급자에 대한 액세스를 제공합니다.\n",
    "8. [AzureOpenAIServerModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.AzureOpenAIServerModel): Azure의 OpenAI 서비스를 사용합니다.\n",
    "9. [AmazonBedrockServerModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.AmazonBedrockServerModel): AWS Bedrock의 API에 연결합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Gemini 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Gemini API 설명서(https://ai.google.dev/gemini-api/docs/openai)에 설명된 대로\n",
    "Google은 Gemini 모델에 대한 OpenAI 호환 API를 제공하므로 적절한 기본 URL을 설정하여 Gemini 모델과 함께 [OpenAIServerModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.OpenAIServerModel)을 사용할 수 있습니다.\n",
    "\n",
    "먼저 필요한 종속성을 설치합니다.\n",
    "```bash\n",
    "pip install smolagents[openai]\n",
    "```\n",
    "\n",
    "그런 다음 [Gemini API 키를 받고](https://ai.google.dev/gemini-api/docs/api-key) 코드에 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = <YOUR-GEMINI-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `OpenAIServerModel` 클래스를 사용하고 `api_base` 매개변수를 Gemini API 기본 URL로 설정하여 Gemini 모델을 초기화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "    model_id=\"gemini-2.0-flash\",\n",
    "    # Google Gemini OpenAI 호환 API 기본 URL\n",
    "    api_base=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=GEMINI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenRouter 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenRouter는 통합된 OpenAI 호환 API를 통해 다양한 언어 모델에 대한 액세스를 제공합니다.\n",
    "적절한 기본 URL을 설정하여 [OpenAIServerModel](https://huggingface.co/docs/smolagents/main/ko/reference/models#smolagents.OpenAIServerModel)을 사용하여 OpenRouter에 연결할 수 있습니다.\n",
    "\n",
    "먼저 필요한 종속성을 설치합니다.\n",
    "```bash\n",
    "pip install smolagents[openai]\n",
    "```\n",
    "\n",
    "그런 다음 [OpenRouter API 키를 받고](https://openrouter.ai/keys) 코드에 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = <YOUR-OPENROUTER-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `OpenAIServerModel` 클래스를 사용하여 OpenRouter에서 사용 가능한 모든 모델을 초기화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "    # OpenRouter에서 사용 가능한 모든 모델 ID를 사용할 수 있습니다.\n",
    "    model_id=\"openai/gpt-4o\",\n",
    "    # OpenRouter API 기본 URL\n",
    "    api_base=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
