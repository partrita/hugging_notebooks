{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "! pip install smolagents\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/smolagents.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`smolagents` provides a flexible framework that allows you to use various language models from different providers.\n",
    "This guide will show you how to use different model types with your agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available model types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`smolagents` supports several model types out of the box:\n",
    "1. [InferenceClientModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.InferenceClientModel): Uses Hugging Face's Inference API to access models\n",
    "2. [TransformersModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.TransformersModel): Runs models locally using the Transformers library\n",
    "3. [VLLMModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.VLLMModel): Uses vLLM for fast inference with optimized serving\n",
    "4. [MLXModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.MLXModel): Optimized for Apple Silicon devices using MLX\n",
    "5. [LiteLLMModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.LiteLLMModel): Provides access to hundreds of LLMs through LiteLLM\n",
    "6. [LiteLLMRouterModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.LiteLLMRouterModel): Distributes requests among multiple models\n",
    "7. [OpenAIServerModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.OpenAIServerModel): Provides access to any provider that implements an OpenAI-compatible API\n",
    "8. [AzureOpenAIServerModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.AzureOpenAIServerModel): Uses Azure's OpenAI service\n",
    "9. [AmazonBedrockServerModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.AmazonBedrockServerModel): Connects to AWS Bedrock's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Gemini Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the Google Gemini API documentation (https://ai.google.dev/gemini-api/docs/openai),\n",
    "Google provides an OpenAI-compatible API for Gemini models, allowing you to use the [OpenAIServerModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.OpenAIServerModel)\n",
    "with Gemini models by setting the appropriate base URL.\n",
    "\n",
    "First, install the required dependencies:\n",
    "```bash\n",
    "pip install smolagents[openai]\n",
    "```\n",
    "\n",
    "Then, [get a Gemini API key](https://ai.google.dev/gemini-api/docs/api-key) and set it in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = <YOUR-GEMINI-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can initialize the Gemini model using the `OpenAIServerModel` class\n",
    "and setting the `api_base` parameter to the Gemini API base URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "    model_id=\"gemini-2.0-flash\",\n",
    "    # Google Gemini OpenAI-compatible API base URL\n",
    "    api_base=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=GEMINI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenRouter Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenRouter provides access to a wide variety of language models through a unified OpenAI-compatible API.\n",
    "You can use the [OpenAIServerModel](https://huggingface.co/docs/smolagents/main/en/reference/models#smolagents.OpenAIServerModel) to connect to OpenRouter by setting the appropriate base URL.\n",
    "\n",
    "First, install the required dependencies:\n",
    "```bash\n",
    "pip install smolagents[openai]\n",
    "```\n",
    "\n",
    "Then, [get an OpenRouter API key](https://openrouter.ai/keys) and set it in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = <YOUR-OPENROUTER-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can initialize any model available on OpenRouter using the `OpenAIServerModel` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "    # You can use any model ID available on OpenRouter\n",
    "    model_id=\"openai/gpt-4o\",\n",
    "    # OpenRouter API base URL\n",
    "    api_base=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
